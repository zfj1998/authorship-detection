from typing import Tuple, List, Dict, Counter

import numpy as np
import pandas as pd

from data_loading.UtilityEntities import PathContext, Path, NodeType, PathContexts
from preprocessing.context_split import ContextSplit, PickType
from util import ProcessedFolder
import ipdb

class PathMinerLoader:
    """
    Loads all .csv files generated by AstMiner into np.ndarray format which is then accessible by PathMinerDataset.
    """
    def __init__(self, project_folder: ProcessedFolder, change_entities: pd.Series, change_to_time_bucket: Dict,
                 min_max_count: Tuple[int, int], author_occurrences: Counter,
                 context_splits: List[ContextSplit] = None):
        self._tokens = self._load_tokens(project_folder.tokens_file)
        self._node_types = self._load_node_types(project_folder.node_types_file)
        self._paths = self._load_paths(project_folder.path_ids_file)
        self._labels, self._path_contexts, self._time_buckets, self._context_indices, self._method_ids = \
            self._load_path_contexts_files(project_folder.file_changes, change_entities, change_to_time_bucket,
                                           min_max_count, context_splits, author_occurrences)
        self._n_classes = np.max(self._labels) + 1
        self._context_depth = 0 if context_splits is None else len(context_splits)

        entities, counts = np.unique(self._labels, return_counts=True)
        ec = [(c, e) for e, c in zip(entities, counts)]
        for i, (c, e) in enumerate(sorted(ec)):
            print(f'{i}: {e} -> {c} | {c / len(self._labels):.4f}')

    def _load_tokens(self, tokens_file: str) -> np.ndarray:
        return self._series_to_ndarray(
            pd.read_csv(tokens_file, sep=',', index_col='id', usecols=['id', 'value'], squeeze=True)
        )

    def _load_paths(self, paths_file: str) -> np.ndarray:
        paths = pd.read_csv(paths_file, sep=',', index_col='id', usecols=['id', 'nodeTypes'], squeeze=True)
        paths = paths.map(
            lambda nt: Path(
                list(map(int, nt.split()))
            )
        )
        return self._series_to_ndarray(paths)

    def _load_node_types(self, node_types_file: str) -> np.ndarray:
        node_types = pd.read_csv(node_types_file, sep=',', index_col='id', usecols=['id', 'type', 'direction'])
        node_types['nodeType'] = node_types.apply(
            lambda row: NodeType(row['type'], row['direction']),
            axis=1
        )
        return self._series_to_ndarray(node_types['nodeType'])

    @staticmethod
    def _load_path_contexts_files(path_contexts_files: List[str], change_entities: pd.Series,
                                  change_to_time_bucket: Dict, min_max_count: Tuple[int, int],
                                  context_splits: List[ContextSplit], author_occurrences: Counter) \
            -> Tuple[np.ndarray, PathContexts, np.ndarray, List[np.ndarray]]:
        '''
        path_contexts_files: ["file_changes_0.csv",...]
        change_entities: {change_id: auther_id, ...} len(change_id): 94962 len(auther_id): 654
        change_to_time_bucket: 
        min_max_count: (1e2, 1e9)
        context_splits: [(depth, {change_id: picktype}),...]
        author_occurrences: {auther_id: count_int} len: 433
        '''
        starts, paths, ends, method_ids = [], [], [], []
        labels = []
        time_buckets = []
        if context_splits is not None:
            # 为每个depth创建一个空列表
            context_indices = [[] for _ in range(len(context_splits))] # len(context_splits) = depths # 10
        else:
            context_indices = None

        for path_contexts_file in path_contexts_files:
            contexts = pd.read_csv(path_contexts_file, sep=',',
                                   usecols=['changeId', 'authorName', 'methodAfterId', 'pathsCountBefore', 'pathsCountAfter', 'pathsAfter'])
            # 🚩仅保留creation的change_id，这一步的筛选在其它文件中做过无数次了，唉
            contexts = contexts[np.logical_and(contexts['pathsCountBefore'] == 0, contexts['pathsCountAfter'] > 0)]
            # 把paths的";"分开，成为许多个path组成的列表
            path_contexts = contexts['pathsAfter'].fillna('').map(
                lambda ctx: np.array(list(map(PathContext.fromstring, ctx.split(';'))), dtype=np.object)
                if ctx
                else np.array([])
            )
            starts.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.start_token, ctx_array), np.int32,
                                              count=ctx_array.size)
            ).values)

            paths.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.path, ctx_array), np.int32, count=ctx_array.size)
            ).values)

            ends.append(path_contexts.map(
                lambda ctx_array: np.fromiter(map(lambda ctx: ctx.end_token, ctx_array), np.int32, count=ctx_array.size)
            ).values)
            # labels也就是author_id
            labels.append(contexts['changeId'].map(
                lambda change_id: change_entities.loc[change_id]
            ).values)
            method_ids.append([i for i in zip(contexts['changeId'], contexts['authorName'], contexts['methodAfterId'])])
            # method_ids.append(contexts['methodAfterId'])

            if change_to_time_bucket is not None:
                time_buckets.append(contexts['changeId'].map(
                    lambda change_id: change_to_time_bucket[change_id]
                ).values)
            # context_indices存的是按depth组织的列表，每个列表都包含了第i个change的分配区域(train,test,ignore)
            if context_splits is not None:
                for i in range(len(context_splits)):
                    context_indices[i].append(contexts['changeId'].map( # 此时的contexts仅包含creation操作的change
                        lambda change_id: context_splits[i].change_to_pick_type[change_id]
                        if change_id in context_splits[i].change_to_pick_type else PickType.IGNORED
                    ))
        # 把几个change_files的东西拼接起来
        starts = np.concatenate(starts)
        paths = np.concatenate(paths)
        ends = np.concatenate(ends)
        labels = np.concatenate(labels)
        method_ids = np.concatenate(method_ids)
        if change_to_time_bucket is not None:
            time_buckets = np.concatenate(time_buckets)
        if context_splits is not None:
            context_indices = [np.concatenate(c) for c in context_indices]

        label_counts = np.array([author_occurrences[label] for label in labels])
        # 🚩这一步又过滤了一遍没有资格的author
        indices = np.logical_and(min_max_count[0] <= label_counts, label_counts <= min_max_count[1])
        starts = starts[indices]
        paths = paths[indices]
        ends = ends[indices]
        labels = labels[indices]
        method_ids = method_ids[indices]
        if change_to_time_bucket is not None:
            time_buckets = time_buckets[indices]
        if context_splits is not None:
            # 过滤掉没有资格的author
            context_indices = [c[indices] for c in context_indices]
        return labels, PathContexts(starts, paths, ends), time_buckets, context_indices, method_ids

    @staticmethod
    def _series_to_ndarray(series: pd.Series) -> np.ndarray:
        converted_values = np.empty(max(series.index) + 1, dtype=np.object)
        for ind, val in zip(series.index, series.values):
            converted_values[ind] = val
        return converted_values

    def tokens(self) -> np.ndarray:
        return self._tokens

    def paths(self) -> np.ndarray:
        return self._paths

    def node_types(self) -> np.ndarray:
        return self._node_types

    def labels(self) -> np.ndarray:
        return self._labels

    def path_contexts(self) -> PathContexts:
        return self._path_contexts

    def time_buckets(self):
        return self._time_buckets

    def context_indices(self, depth):
        return self._context_indices[depth]

    def n_classes(self) -> int:
        return self._n_classes

    def context_depth(self) -> int:
        return self._context_depth
    
    def method_ids(self) -> np.ndarray:
        return self._method_ids
